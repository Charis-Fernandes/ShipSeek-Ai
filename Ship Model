{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"collapsed_sections":["EvxPB-mqmFrW","wATcP3FVmFre","75TVsB85mFre","U0tL3mZemFrj"]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9988,"databundleVersionId":868324,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":">  **SHIPSEEK-AI** ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom skimage.io import imread\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util import montage\nfrom skimage.morphology import label\n\nimport gc\ngc.enable()","metadata":{"id":"duMa5oo7mFrI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exploring the data <a class=\"anchor\"  id=\"h3\"></a>","metadata":{"id":"crJdN2a9mFrO"}},{"cell_type":"code","source":"# Train and test directories\ntrain_image_dir = '../input/airbus-ship-detection/train_v2'\ntest_image_dir = \"../input/airbus-ship-detection/test_v2\"","metadata":{"id":"q1XfqoRvmFrP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Getting into train directory\ntrain_images = os.listdir(train_image_dir)\ntrain_images.sort()\nprint(f\"Total of {len(train_images)} images in train directory.\\nHere is how first five train_images looks like:- {train_images[:5]}\")","metadata":{"id":"_v7uprIWmFrP","outputId":"2a11ea28-c293-408e-acf2-7b4cc2fe319a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using for loop to generate different images to understand how data looks like\nplt.figure(figsize=(15,15))\nplt.suptitle('TRAIN IMAGES\\n', weight = 'bold', fontsize = 15, color = 'r')\nfor i in range(16):\n    plt.subplot(4, 4, i+1)\n    plt.imshow(imread(train_image_dir + \"/\" + train_images[i]))\n    plt.title(f\"{train_images[i]}\", weight = 'bold')\n    plt.axis('off')\nplt.tight_layout()","metadata":{"id":"2IfuQJbDmFrQ","outputId":"4594231b-4728-4b56-fb79-57f85b7541a5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- So there are many images here that has no ships and few that does have one or multiple ships.","metadata":{"id":"6IUkSmz-mFrR"}},{"cell_type":"code","source":"# Train ships segmented masks\nmasks = pd.read_csv(\"../input/airbus-ship-detection/train_ship_segmentations_v2.csv\")\nmasks.head(10)","metadata":{"id":"zoIy1ed2mFrR","outputId":"58dfddd8-5ac8-417a-e644-e4424d88a4ae","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row_rle = ['10 1', \n           '4 1 2 0 4 1', \n           '3 1 4 0 3 1', \n           '2 1 6 0 2 1',\n           '1 1 2 0 1 1 2 0 1 1 2 0 1 1', \n           '1 1 8 0 1 1', \n           '3 1 1 0 2 1 1 0 3 1', \n           '2 1 1 0 1 1 2 0 1 1 1 0 2 1', \n           '1 1 1 0 1 1 1 0 2 1 1 0 1 1 1 0 1 1', \n           '10 1',\n           'Total']\n\npixels = [len(row.split(\" \")) for row in row_rle if row != 'Total']\nsum_pixels = np.array(pixels).sum()\npixels.append(sum_pixels)\n\ndata = {\n    'Row - RLE' : row_rle,\n    'Pixels' : pixels\n}\n\nrle_df = pd.DataFrame(data)\nrle_df.index+=1\nrle_df\n","metadata":{"id":"96TekMv2mFrS","outputId":"ea74af11-f7e3-4383-f9fd-8a790e118f79","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Clearly we have compressed 100 data points into 84 data points.\n- Some rows like 5, 8 and 14 gave more data points after encoding than it was in original.\n- Reason being short runs as there were multiple breaks between black and white pixels.\n- We can now apply this idea onto our data!","metadata":{"id":"85s6TxoEmFrS"}},{"cell_type":"code","source":"# Let us now see how it works for Image id:- 0005d01c8.jpg we have in the mask data frame\n\n# Original image from training set\nimg_arr = imread(train_image_dir + '/' + '0005d01c8.jpg')\nplt.figure(figsize=(15,8))\nplt.imshow(img_arr)\nplt.show()","metadata":{"id":"ZqlG5qDbmFrS","outputId":"36b8e417-8250-4876-e940-61981f73d79b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_arr.shape","metadata":{"id":"IJhFOljtmFrT","outputId":"3d10f1b8-5828-4635-e7ee-c6c581ddb464","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter out all 0005d01c8.jpg image ids and respective encoded data \n# 2 ships means 2 same image ids will be there!\nrle_0 = masks.query('ImageId==\"0005d01c8.jpg\"')['EncodedPixels']\nrle_0","metadata":{"id":"dz6on9OLmFrT","outputId":"98fcaf86-a88e-43e4-9f66-8ed365bb0f25","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make a list of each mask shown above also visualise whats happening!\nmask_lst, ct = [], 1\nfor mask in rle_0:\n    print(f\"Mask {ct} -\\n{mask}\\n\\n\")\n    mask_lst.append(mask)\n    ct+=1","metadata":{"id":"0ccDO-J0mFrT","outputId":"a262b896-b911-4abd-b8ab-f68d70fc603d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split and Display how the first mask in the list looks like\nsplit = mask_lst[0].split()\nprint(split)","metadata":{"id":"JcXAQabMmFrU","outputId":"8f589ee8-5bd6-4878-cc8b-659f04dcf323","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- This data shows start_pixels and lenghts where we can think ship to exist in the original image.\n- For example, 56777 3 shows that pixels 56777, 56778, 56779 contributes to the ship.\n- Our target is to create an image with these pixels labeled as 1 and remaining as 0.\n- This is how we can produce a mask for respective image.","metadata":{"id":"0lTvIaOMmFrU"}},{"cell_type":"code","source":"# Grab all the starting pixels and lenghts and convert it into integers using numpy \nstarts, lengths = [np.array(x, dtype = int) for x in (split[::2], split[1::2])]\nstarts, lengths","metadata":{"id":"JYxNhVTemFrU","outputId":"c8faed91-d89e-4384-eb49-3d80f3f7df95","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the ending pixels. \n'''Examples:- \n56010 1 ---> Starts at 56010 and ends at 56010\n56777 3 ---> Starts at 56777 and ends at 56779\n57544 6 ---> Starts at 57544 and ends at 57549'''\nends = starts + lengths - 1\npd.DataFrame({\n    'Starts' : starts,\n    'Lengths' : lengths,\n    'Ends' : ends\n}).head(10)","metadata":{"id":"m_jYpfHymFrU","outputId":"7eebca8d-5238-4a94-90bb-4866838f67b3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create 1s in place of these pixels and rest should be 0\nimg = np.zeros(768*768, dtype = np.uint8)\nfor start, end in zip(starts, ends):\n    img[start:end+1] = 1","metadata":{"id":"5gXIUsCOmFrV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check how output looks\nimg[56776:56781] # Should output 0, 1 , 1, 1 ,0 as we know 56777, 56778, 56779 ---> 1 and 5676, 56780 ---> 0","metadata":{"id":"wyzWtoRnmFrV","outputId":"08a86a66-609a-44d4-f689-b613a1fa92bb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Copy-Paste this idea for another ship in the image\nsplit_1 = mask_lst[1].split()                                                                # Split the mask into start_pixels and lengths\nstarts, lengths = [np.array(x, dtype = int) for x in (split_1[0:][::2], split_1[1:][::2])]   # Generate arrays from only starts and lengths\nends = starts + lengths - 1                                                                  # Start pixel to end pixel will be start - 1 + length\nimg1 = np.zeros(768*768, dtype = np.uint8)                                                   # 1D array containing all zeros\nfor start, end in zip(starts, ends):                                                         # For each start to end pair\n    img1[start:end+1] = 1                                                                    # Convert the values from 0 to 1","metadata":{"id":"EzpbR0zNmFrV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reshaping both the ship masks and combining it to form the final mask!\nimg = img.reshape(768, 768)\nimg1 = img1.reshape(768, 768)\nfinal = img+img1\nprint(final, '\\n\\n', final.shape, '\\n\\n', final.ndim)","metadata":{"id":"GaFH_dzQmFrV","outputId":"3bf72b85-38a8-4392-aace-437c46e4a9c3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Expand dimension of this array to have only 1 channel in the mask and visualise original and final mask\nfinal = np.expand_dims(final, -1) # -1 means the last available dimenstion, in this case it is 2. Hence, on axis = 2 we will get 1.\noriginal = imread(train_image_dir+'/'+train_images[15])\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1)\nplt.title(f\"Original - Train Image, {original.shape}\")\nplt.imshow(original)\nplt.subplot(1, 2, 2)\nplt.title(f\"Mask generated from the RLE data for each ship, {final.shape}\")\nplt.imshow(final, cmap = \"gray\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"GQf7ZZ3XmFrW","outputId":"5d90b3bd-4511-4769-9b82-67c79c8fd62a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Oh, something is off!\n- Our mask needs to be transposed. ","metadata":{"id":"6kZifw3AmFrW"}},{"cell_type":"code","source":"# Copy Paste the code from the prev cell with one change - Transpose!\nimg = img.reshape(768, 768).T     # Transpose the first ship mask\nimg1 = img1.reshape(768, 768).T   # Transpose the second ship mask\nfinal = img+img1                  # Generate the final mask with two ships \nfinal = np.expand_dims(final, -1) \nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1)\nplt.title(f\"Original - Train Image, {original.shape}\")\nplt.imshow(original)\nplt.subplot(1, 2, 2)\nplt.title(f\"Mask generated from the RLE data for each ship, {final.shape}\")\nplt.imshow(final, cmap = \"Blues_r\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"jZrbe3d6mFrW","outputId":"530150c2-e503-401c-8fbf-3653cbd2c77e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- So this is how the EncodedPixels data for one image id looks like!\n- We can build a function that can quickly generate such masks for all the EncodedPixels wrt to its ImageId.","metadata":{"id":"ccK3v3elmFrW"}},{"cell_type":"code","source":"# Define functions to do these tasks for all the training images\ndef rle_decode(mask_rle, shape=(768,768)):\n    '''\n    Input arguments -\n    mask_rle: Mask of one ship in the train image\n    shape: Output shape of the image array\n    '''\n    s = mask_rle.split()                                                               # Split the mask of each ship that is in RLE format\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]     # Get the start pixels and lengths for which image has ship\n    ends = starts + lengths - 1                                                        # Get the end pixels where we need to stop\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)                                  # A 1D vec full of zeros of size = 768*768\n    for lo, hi in zip(starts, ends):                                                   # For each start to end pixels where ship exists\n        img[lo:hi+1] = 1                                                               # Fill those values with 1 in the main 1D vector\n    '''\n    Returns -\n    Transposed array of the mask: Contains 1s and 0s. 1 for ship and 0 for background\n    '''\n    return img.reshape(shape).T                                                       \n\ndef masks_as_image(in_mask_list):\n    '''\n    Input - \n    in_mask_list: List of the masks of each ship in one whole training image\n    '''\n    all_masks = np.zeros((768, 768), dtype = np.int16)                                 # Creating 0s for the background\n    for mask in in_mask_list:                                                          # For each ship rle data in the list of mask rle \n        if isinstance(mask, str):                                                      # If the datatype is string\n            all_masks += rle_decode(mask)                                              # Use rle_decode to create one mask for whole image\n    '''\n    Returns - \n    Full mask of the training image whose RLE data has been passed as an input\n    '''\n    return np.expand_dims(all_masks, -1)","metadata":{"id":"dPjmkwHjmFrW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for num in [3, 4, 5, 6]:\n    rle_0 = masks.query(f'ImageId==\"{train_images[num-1]}\"')['EncodedPixels']\n    img_0 = masks_as_image(rle_0)\n    original = imread(train_image_dir+\"/\"+train_images[num-1])\n    plt.figure(figsize=(15, 8))\n    plt.subplot(1, 2, 1)\n    plt.title(f\"Original - Train Image {original.shape}\")\n    plt.imshow(original)\n    plt.subplot(1, 2, 2)\n    plt.title(f\"Mask generated from the RLE data for each ship {final.shape}\")\n    plt.imshow(img_0, cmap = \"Blues_r\")\n    plt.tight_layout()\n    plt.show()","metadata":{"id":"N3JLeU2ps5zG","outputId":"3a33c31e-e02a-4e87-dd62-de750de7707e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- We have succesfully constructed some functions that will take in the rle data and convert it into mask!\n- We can now begin with spliting the data into train and validation.\n\n## Preparing Train and Validation Data <a class=\"anchor\"  id=\"h5\"></a>","metadata":{"id":"EvxPB-mqmFrW"}},{"cell_type":"code","source":"'''Note that NaN values in the EncodedPixels are of float type and everything else is a string type'''   \n\n# Add a new feature to the masks data frame named as ship. If Encoded pixel in any row is a string, there is a ship else there isn't. \nmasks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\nmasks.head(9)","metadata":{"id":"h0BarNjlmFrX","outputId":"f5207375-1076-4703-81c7-b77c4f55f013","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Making a new data frame with unique image ids where we are summing up the ship counts\nunique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index() \nunique_img_ids.index+=1 # Incrimenting all the index by 1\nunique_img_ids.head()","metadata":{"id":"iW_hlCevmFrX","outputId":"5c2f0605-a7f1-4018-c2db-e231138a817c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adding two new features to unique_img_ids data frame. If ship exists in image, val is 1 else 0. And it's vec form\nunique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\nunique_img_ids.head()","metadata":{"id":"aPaLVit8mFrX","outputId":"76a04e17-fa8c-4a27-fbdf-42f63a6fd876","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the size of the files. Will take some time to run as there are loads of files!!!\nunique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: os.stat(os.path.join(train_image_dir, c_img_id)).st_size/1024)\n'''os.stat is used to get status of the specified path. Here, st_size represents size of the file in bytes. Converting it into kB!'''","metadata":{"id":"ETbI2Byss5zG","outputId":"ed08ef0c-db14-46ed-8fab-30af19a35c3b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We can get rid of any images whose size is less than 35 Kb. As some of the files are corrupted! \nunique_img_ids[unique_img_ids.file_size_kb<35].head()","metadata":{"id":"dh7Z40aIs5zH","outputId":"c86e02f4-f0af-4561-d8de-e9d827cbd48b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rle_0 = masks.query(f'ImageId==\"0318fc519.jpg\"')['EncodedPixels']\nimg_0 = masks_as_image(rle_0)\noriginal = imread(train_image_dir+\"/\"+'0318fc519.jpg')\nplt.figure(figsize=(15, 8))\nplt.subplot(1, 2, 1)\nplt.title(f\"Original - Train Image {original.shape}\")\nplt.imshow(original)\nplt.subplot(1, 2, 2)\nplt.title(f\"Mask generated from the RLE data for each ship {final.shape}\")\nplt.imshow(img_0, cmap = \"Blues_r\")\nplt.tight_layout()\nplt.show()","metadata":{"id":"nYF_0MYfmFrY","outputId":"cf664010-00ee-432a-dfdc-2f7b79705d98","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keep the files whose size > 35 kB\nunique_img_ids = unique_img_ids[unique_img_ids.file_size_kb > 35]\nunique_img_ids.head()","metadata":{"id":"Vcom6VhkmFrY","outputId":"022411aa-a97c-4e5d-a31d-02ea013dd24b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Also, retrive the old masks data frame\nmasks.drop(['ships'], axis=1, inplace=True)\nmasks.index+=1 \nmasks.head()","metadata":{"id":"DfJCvsCHmFrY","outputId":"43d5f845-b7a9-4c85-861d-4eb5dd059880","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Now, its the time to use the train_test_split.\n- Stratify to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset.","metadata":{"id":"zCJ9sHZgmFrY"}},{"cell_type":"code","source":"# Train - Test split\nfrom sklearn.model_selection import train_test_split                   \ntrain_ids, valid_ids = train_test_split(unique_img_ids, test_size = 0.3, stratify = unique_img_ids['ships'])","metadata":{"id":"bbQHbyzxmFrY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create train data frame\ntrain_df = pd.merge(masks, train_ids)\n\n# Create test data frame\nvalid_df = pd.merge(masks, valid_ids)","metadata":{"id":"mObH089jmFrZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"There are ~\")\nprint(train_df.shape[0], 'training masks,')\nprint(valid_df.shape[0], 'validation masks.')","metadata":{"id":"5Po1UEcGmFrZ","outputId":"0bbb1cb4-603c-43d2-97a6-a2647072d42a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualise the ship counts\nplt.figure(figsize=(10, 6))\nsns.countplot(train_df.ships)\nplt.show()","metadata":{"id":"hGt5Qns4mFrZ","outputId":"19bbeba3-2baf-450e-d908-3770ea8b439f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Oh!! Very huge imbalance in the data...\n- We need to have a better balanced data!!!","metadata":{"id":"gISXjCmZmFrd"}},{"cell_type":"markdown","source":"## Random Undersampling to generate a better balanced data to work with","metadata":{"id":"wATcP3FVmFre"}},{"cell_type":"code","source":"# Clipping the max value of grouped_ship_count to be 7, minimum to be 0\ntrain_df['grouped_ship_count'] = train_df.ships.map(lambda x: (x+1)//2).clip(0,7)","metadata":{"id":"GW1DYwrfmFre","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check\ntrain_df.grouped_ship_count.value_counts()","metadata":{"id":"QHic4kjdmFre","outputId":"900177b5-aff2-4aba-ab08-fc1124e93356","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Difference between head(n) and sample(n) in pandas\n- df.head(n) returns only top n data from the df\n- df.sample(n) returns random n data from the df","metadata":{"execution":{"iopub.status.busy":"2022-10-07T10:50:30.915451Z","iopub.execute_input":"2022-10-07T10:50:30.916382Z","iopub.status.idle":"2022-10-07T10:50:30.941993Z","shell.execute_reply.started":"2022-10-07T10:50:30.916340Z","shell.execute_reply":"2022-10-07T10:50:30.940812Z"},"id":"75TVsB85mFre"}},{"cell_type":"code","source":"# Top 10 data\ntrain_df.head(10)","metadata":{"id":"pnr2H5_WmFre","outputId":"c657d012-f519-44f4-ae97-e46928698014","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random 10 data\ntrain_df.sample(10)","metadata":{"id":"lHfspu_7mFre","outputId":"8da9cdfb-1932-472d-c844-78a5e427dbba","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Random Under-Sampling ships\ndef sample_ships(in_df, base_rep_val=1500):\n    '''\n    Input Args:\n    in_df - dataframe we want to apply this function\n    base_val - random sample of this value to be taken from the data frame\n    '''\n    if in_df['ships'].values[0]==0:                                                 \n        return in_df.sample(base_rep_val//3)  # Random 1500//3 = 500 samples taken whose ship count is 0 in an image \n    else:                                 \n        return in_df.sample(base_rep_val)    # Random 1500 samples taken whose ship count is not 0 in an image","metadata":{"id":"yvk146BImFre","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating groups of ship counts and applying the sample_ships functions to randomly undersample the ships\nbalanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\nbalanced_train_df.grouped_ship_count.value_counts() # In each group we have total of 1500 ships except 0 as we have decreased it even more to 500","metadata":{"id":"Sh2SgP5lmFrf","outputId":"52adb1b2-7c42-4e5a-9275-dc61f8aee600","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explaining what we just did if still not clear\nfor i in range(8):\n    df_val_counts = balanced_train_df[balanced_train_df.grouped_ship_count==i].ships.value_counts()\n    print(f\"Data frame for grouped ship count = {i}:-\\n{df_val_counts}\\nSum of Values:- {df_val_counts.values.sum()}\\n\\n\")\n","metadata":{"id":"6SERf32pmFrf","outputId":"612c6483-a49f-4cf1-d6c2-cd3a5486bfb6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(balanced_train_df.head())\nprint(balanced_train_df['ships'].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (15, 5))\nplt.suptitle(\"Train Data\", fontsize = 18, color = 'r', weight = 'bold')\nplt.subplot(1, 2, 1)\nimport seaborn as sns\nsns.countplot(train_df.ships, palette = 'Set2')\nplt.title(\"Ship Counts - Before Balancing\", color = 'm', fontsize = 15)\nplt.ylabel(\"Count\", color = 'tab:pink', fontsize = 13)\nplt.xlabel(\"# Ships in an image\", color = 'tab:pink', fontsize = 13)\nplt.subplot(1, 2, 2)\nsns.countplot(balanced_train_df.ships, palette = 'Set2')\nplt.title(\"Ship Counts - After Balancing\", color = 'm', fontsize = 15)\nplt.xlabel(\"# Ships in an image\", color = 'tab:pink', fontsize = 13)\nplt.tight_layout()","metadata":{"id":"XnLI4bdHmFrf","outputId":"42aba9de-7844-4989-9059-d1b688d8b0cb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parameters\nBATCH_SIZE = 4                 # Train batch size\nEDGE_CROP = 16                 # While building the model\nNB_EPOCHS = 5                  # Training epochs\nGAUSSIAN_NOISE = 0.1           # To be used in a layer in the model\nUPSAMPLE_MODE = 'SIMPLE'       # SIMPLE ==> UpSampling2D, else Conv2DTranspose\nNET_SCALING = None             # Downsampling inside the network                        \nIMG_SCALING = (1, 1)           # Downsampling in preprocessing\nVALID_IMG_COUNT = 400          # Valid batch size\nMAX_TRAIN_STEPS = 200          # Maximum number of steps_per_epoch in training","metadata":{"id":"YnVyTCqUmFrf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_image_gen(in_df, batch_size=BATCH_SIZE):\n    \"\"\"\n    Generator that yields batches of images and masks.\n    Output shapes:\n      - Images: (batch_size, H, W, 3) normalized float32 values.\n      - Masks: (batch_size, H, W, 1) float32 values.\n    \"\"\"\n    all_batches = list(in_df.groupby('ImageId'))\n    out_rgb, out_mask = [], []\n    \n    while True:\n        np.random.shuffle(all_batches)\n        for c_img_id, c_masks in all_batches:\n            rgb_path = os.path.join(train_image_dir, c_img_id)\n            # Read and convert image to float32\n            c_img = imread(rgb_path)\n            if c_img is None:\n                continue\n            c_img = c_img.astype(np.float32)\n            \n            # Convert RLE masks to an image array\n            c_mask = masks_as_image(c_masks['EncodedPixels'].values)\n            c_mask = np.array(c_mask).astype(np.float32)\n            \n            # If the mask is 2D, add a channel dimension.\n            if c_mask.ndim == 2:\n                c_mask = np.expand_dims(c_mask, axis=-1)\n            # If the mask has an extra dimension (e.g. shape (H, W, 1, 1)), remove it.\n            if c_mask.ndim == 4 and c_mask.shape[-1] == 1:\n                c_mask = np.squeeze(c_mask, axis=-1)\n            \n            out_rgb.append(c_img)\n            out_mask.append(c_mask)\n            \n            if len(out_rgb) >= batch_size:\n                batch_x = np.stack(out_rgb) / 255.0   # Normalize images to [0,1]\n                batch_y = np.stack(out_mask)            # Stack masks\n                \n                # If batch_y still has an extra dimension (e.g. (B,H,W,1,1)), squeeze it.\n                if batch_y.ndim == 5:\n                    batch_y = np.squeeze(batch_y, axis=-1)  # Now shape should be (B, H, W, 1)\n                yield batch_x, batch_y\n                out_rgb, out_mask = [], []","metadata":{"id":"_l69s0W_mFrf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gen = make_image_gen(balanced_train_df)\nxb, yb = next(gen)\nprint(\"✅ x shape:\", xb.shape, xb.dtype)  # Expected: (batch_size, 768, 768, 3), float32\nprint(\"✅ y shape:\", yb.shape, yb.dtype)  # Expected: (batch_size, 768, 768, 1), float32\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate train data \ntrain_gen = make_image_gen(balanced_train_df)\n\n# Image and Mask\ntrain_x, train_y = next(train_gen)\n\n# Print the summary\nprint(f\"train_x ~\\nShape: {train_x.shape}\\nMin value: {train_x.min()}\\nMax value: {train_x.max()}\")\nprint(f\"\\ntrain_y ~\\nShape: {train_y.shape}\\nMin value: {train_y.min()}\\nMax value: {train_y.max()}\")","metadata":{"id":"LrGDDsHvmFrf","outputId":"df2fec31-16f5-4777-ade6-c3e79a8eabf6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visulaising train batch\n# Helper to create an RGB montage\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n\n# Create montages\nbatch_rgb = montage_rgb(train_x)                          # Shape: (H', W', 3)\nbatch_seg = montage(train_y.squeeze())                    # Shape: (H', W') – squeezed mask\nbatch_overlap = mark_boundaries(batch_rgb, batch_seg.astype(int))\n\n# Plotting\ntitles = [\"Images\", \"Segmentations\", \"Bounding Boxes on ships in Images\"]\ncolors = ['g', 'm', 'b']\ndisplay = [batch_rgb, batch_seg, batch_overlap]\n\nplt.figure(figsize=(25, 10))\nfor i in range(3):\n    plt.subplot(1, 3, i+1)\n    plt.imshow(display[i])\n    plt.title(titles[i], fontsize=18, color=colors[i])\n    plt.axis('off')\n\nplt.suptitle(\"Batch Visualizations\", fontsize=20, color='r', weight='bold')\nplt.tight_layout()\nplt.show()                                                               # Layout for subplot","metadata":{"id":"YeeCgg_PmFrf","outputId":"dca020dc-5290-4100-b206-6f277a1e0506","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare validation data\nvalid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\nprint(f\"valid_x ~\\nShape: {valid_x.shape}\\nMin value: {valid_x.min()}\\nMax value: {valid_x.max()}\")\nprint(f\"\\nvalid_y ~\\nShape: {valid_y.shape}\\nMin value: {valid_y.min()}\\nMax value: {valid_y.max()}\")","metadata":{"id":"aUXAhwWKmFrg","outputId":"4c4f0973-9452-495c-b696-b0dbe07debcd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Augmenting Data using ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Preparing image data generator arguments\ndg_args = dict(rotation_range = 15,            # Degree range for random rotations\n               horizontal_flip = True,         # Randomly flips the inputs horizontally\n               vertical_flip = True,           # Randomly flips the inputs vertically\n               data_format = 'channels_last')  # channels_last refer to (batch, height, width, channels)","metadata":{"id":"Ru71AFFwmFrg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Click [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to find more information on ImageDataGenerator and its arguments.","metadata":{"id":"SZcUkxocmFrg"}},{"cell_type":"code","source":"image_gen = ImageDataGenerator(**dg_args)\nlabel_gen = ImageDataGenerator(**dg_args)\n\ndef create_aug_gen(in_gen, seed=None):\n    \"\"\"\n    Create an augmented generator that yields batches of images and masks.\n    Expects `in_gen` to yield (in_x, in_y) where in_y has shape (batch, H, W, 1).\n    \"\"\"\n    for in_x, in_y in in_gen:\n        # Create separate ImageDataGenerators for images and masks\n        image_gen = ImageDataGenerator(\n            rotation_range=15,\n            horizontal_flip=True,\n            vertical_flip=True\n        )\n        label_gen = ImageDataGenerator(\n            rotation_range=15,\n            horizontal_flip=True,\n            vertical_flip=True\n        )\n\n        # Use the same seed to ensure the augmentations are applied identically\n        g_x = image_gen.flow(in_x, batch_size=in_x.shape[0], seed=seed, shuffle=False)\n        g_y = label_gen.flow(in_y, batch_size=in_x.shape[0], seed=seed, shuffle=False)\n        \n        yield next(g_x), next(g_y)\n","metadata":{"id":"HQvlfWvjmFrg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Augment the train data\ncur_gen = create_aug_gen(train_gen, seed = 42)\nt_x, t_y = next(cur_gen)\nprint('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\nprint('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())","metadata":{"id":"YChZ_zOqmFrg","outputId":"a941718e-7822-48f5-ee72-d37771ecec6a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Remove all singleton dimensions from t_y\nmask_batch = np.squeeze(t_y)\n# If the result is 2D (e.g. batch size was 1), add the batch dimension back.\nif mask_batch.ndim == 2:\n    mask_batch = np.expand_dims(mask_batch, axis=0)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(25, 10))\n\n# Display Images montage using your helper for RGB images\nax1.imshow(montage_rgb(t_x), cmap='gray')\nax1.set_title('Images', fontsize=18, color='g')\nax1.axis('off')\n\n# Display the Masks montage: mask_batch should now be 3D: (N, H, W)\nax2.imshow(montage(mask_batch), cmap='Blues_r')\nax2.set_title('Masks', fontsize=18, color='r')\nax2.axis('off')\n\n# For overlay, create the montage of images and masks\nax3.imshow(mark_boundaries(montage_rgb(t_x), montage(mask_batch).astype(int)))\nax3.set_title('Bounding Box', fontsize=18, color='b')\nax3.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"id":"Tg7MNJdOmFrg","outputId":"c8c40493-674e-49d1-e9fa-bd281b33470c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect() # Block all the garbage that has been generated","metadata":{"id":"H2GlhnR-mFrh","outputId":"0a87d09b-5b3a-45d2-fa71-7afdf43b8721","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**U-NET**","metadata":{}},{"cell_type":"code","source":"# Build U-Net model\nfrom keras import models, layers\n\n# Conv2DTranspose upsampling\ndef upsample_conv(filters, kernel_size, strides, padding):\n    return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n# Upsampling without Conv2DTranspose\ndef upsample_simple(filters, kernel_size, strides, padding):\n    return layers.UpSampling2D(strides)\n\n# Upsampling method choice\nif UPSAMPLE_MODE=='DECONV':\n    upsample=upsample_conv\nelse:\n    upsample=upsample_simple\n\n# Building the layers of UNET\ninput_img = layers.Input(t_x.shape[1:], name = 'RGB_Input')\npp_in_layer = input_img\n\n# If NET_SCALING is defined then do the next step else continue ahead\nif NET_SCALING is not None:\n    pp_in_layer = layers.AvgPool2D(NET_SCALING)(pp_in_layer)\n\n# To avoid overfitting and fastening the process of training\npp_in_layer = layers.GaussianNoise(GAUSSIAN_NOISE)(pp_in_layer)                       # Useful to mitigate overfitting\npp_in_layer = layers.BatchNormalization()(pp_in_layer)                                # Allows using higher learning rate without causing problems with gradients\n\n\n## Downsample (C-->C-->MP)\n\nc1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (pp_in_layer)\nc1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\np1 = layers.MaxPooling2D((2, 2)) (c1)\n\nc2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\nc2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\np2 = layers.MaxPooling2D((2, 2)) (c2)\n\nc3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\nc3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\np3 = layers.MaxPooling2D((2, 2)) (c3)\n\nc4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\nc4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\np4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n\n\nc5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\nc5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n\n## Upsample (U --> Concat --> C --> C)\n\nu6 = upsample(64, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = layers.concatenate([u6, c4])\nc6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\nc6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n\nu7 = upsample(32, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = layers.concatenate([u7, c3])\nc7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\nc7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n\nu8 = upsample(16, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = layers.concatenate([u8, c2])\nc8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\nc8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n\nu9 = upsample(8, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = layers.concatenate([u9, c1], axis=3)\nc9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\nc9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n\nd = layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\nd = layers.Cropping2D((EDGE_CROP, EDGE_CROP))(d)\nd = layers.ZeroPadding2D((EDGE_CROP, EDGE_CROP))(d)\n\nif NET_SCALING is not None:\n    d = layers.UpSampling2D(NET_SCALING)(d)\n\nseg_model = models.Model(inputs=[input_img], outputs=[d])\n\nseg_model.summary()","metadata":{"id":"Wb1YIcPTmFrh","outputId":"3b67adaa-872e-4943-c435-79b23331b253","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute dice coefficient, loss with BCE and compile the model\nimport keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\n\n# Dice coeff\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3]  )                         # int = y_true ∩ y_pred\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])           # un = y_true_flatten ed ∪ y_pred_flattened\n    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)     # dice = 2 * int + 1 / un + 1\n\n# Dice with BCE\ndef dice_p_bce(y_true, y_pred):\n    '''\n    Compute this function based on the explanation\n    - use alpha = 1e-3\n    '''\n    \n    combo_loss = \"Something\"\n         \n    return combo_loss\n\n# Compile the model\nseg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef])","metadata":{"id":"esAWI3D_mFrh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preparing Callbacks \nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n\n# Best model weights\nweight_path = \"{}.weights.h5\".format('seg_model')\n\n\n# Monitor validation dice coeff and save the best model weights\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\n\n# Reduce Learning Rate on Plateau\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n                                   patience=3, \n                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n\n# Stop training once there is no improvement seen in the model\nearly = EarlyStopping(monitor=\"val_dice_coef\", \n                      mode=\"max\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\n\n# Callbacks ready\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","metadata":{"id":"Oy1k7BWEmFrh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- *Just in case you are not aware about callbacks we use in keras you can learn more about it [here](https://keras.io/api/callbacks/).*","metadata":{"id":"qH2NrpmNs5zL"}},{"cell_type":"code","source":"# Finalizing steps per epoch\nstep_count = min(MAX_TRAIN_STEPS, balanced_train_df.shape[0]//BATCH_SIZE)\n\n# Final augmented data being used in training\naug_gen = create_aug_gen(make_image_gen(balanced_train_df))\n\n# Save loss history while training\nloss_history = [seg_model.fit(\n    aug_gen,\n    steps_per_epoch=step_count,\n    epochs=NB_EPOCHS,\n    validation_data=(valid_x, valid_y),\n    callbacks=callbacks_list\n)]\n\n","metadata":{"id":"4QNJtGrRmFrj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the weights to load it later for test data \n# Skip loading since weights don't exist yet\nseg_model.save(\"/kaggle/working/model.h5\")\nprint(\"✅ Model saved!\")\n","metadata":{"id":"puTYhSN0mFrj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"New 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─────────────────────────────────────────────\n# 1) Imports & Paths\n# ─────────────────────────────────────────────\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nBASE_DIR      = \"/kaggle/input/airbus-ship-detection\"\nTRAIN_DIR     = os.path.join(BASE_DIR, \"train_v2\")\nCSV_PATH      = os.path.join(BASE_DIR, \"train_ship_segmentations_v2.csv\")\nBATCH_SIZE    = 8\nIMG_SIZE      = 384\nEPOCHS        = 5\nWEIGHT_PATH   = \"seg_model.weights.h5\"\nOUTPUT_MODEL  = \"/kaggle/working/model.h5\"\n\n# ─────────────────────────────────────────────\n# 2) Load CSV & RLE decode utility\n# ─────────────────────────────────────────────\ndf = pd.read_csv(CSV_PATH)\n\ndef rle_decode(mask_rle, shape=(IMG_SIZE, IMG_SIZE)):\n    \"\"\"Mask RLE decoder.\"\"\"\n    if pd.isnull(mask_rle): \n        return np.zeros(shape, dtype=np.uint8)\n    s = list(map(int, mask_rle.split()))\n    starts, lengths = s[0::2], s[1::2]\n    starts = np.array(starts, dtype=int) - 1\n    ends   = starts + np.array(lengths, dtype=int)\n    img     = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\n# ─────────────────────────────────────────────\n# 3) Prepare list of images + aggregated RLE lists\n# ─────────────────────────────────────────────\ngrp = df.groupby(\"ImageId\")[\"EncodedPixels\"].agg(lambda x: list(x.dropna()))\nvalid = grp.reset_index().rename(columns={\"EncodedPixels\":\"RLEs\"})\n# filter only images that actually have at least one ship\nvalid = valid[ valid[\"RLEs\"].map(len) > 0 ].reset_index(drop=True)\n\n# ─────────────────────────────────────────────\n# 4) Train/Validation Split\n# ─────────────────────────────────────────────\ntrain_ids, val_ids = train_test_split(\n    valid[\"ImageId\"].tolist(), test_size=0.1, random_state=42\n)\ntrain_df = valid[ valid[\"ImageId\"].isin(train_ids) ].reset_index(drop=True)\nval_df   = valid[ valid[\"ImageId\"].isin(val_ids)   ].reset_index(drop=True)\n\n# ─────────────────────────────────────────────\n# 5) Data Generator (with augmentation)\n# ─────────────────────────────────────────────\ndef make_gen(df):\n    ids = df[\"ImageId\"].tolist()\n    while True:\n        random.shuffle(ids)\n        for i in range(0, len(ids), BATCH_SIZE):\n            batch_ids = ids[i : i + BATCH_SIZE]\n            X, Y = [], []\n            for img_id in batch_ids:\n                # load + preprocess image\n                img = cv2.imread(os.path.join(TRAIN_DIR, img_id))\n                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)).astype(np.float32) / 255.0\n                # decode and combine all RLEs for this image\n                mask = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n                for rle in df[df.ImageId == img_id][\"RLEs\"].iloc[0]:\n                    mask |= rle_decode(rle, (IMG_SIZE, IMG_SIZE))\n                mask = np.expand_dims(mask, -1).astype(np.float32)\n                X.append(img); Y.append(mask)\n            yield np.stack(X), np.stack(Y)\n\ndef create_aug_gen(base_gen, seed=42):\n    img_aug = ImageDataGenerator(\n        rotation_range=15, horizontal_flip=True, vertical_flip=True\n    )\n    msk_aug = ImageDataGenerator(\n        rotation_range=15, horizontal_flip=True, vertical_flip=True\n    )\n    for Xb, Yb in base_gen:\n        # ensure same seed for sync transforms\n        gX = img_aug.flow(Xb, batch_size=Xb.shape[0], seed=seed, shuffle=False)\n        gY = msk_aug.flow(Yb, batch_size=Xb.shape[0], seed=seed, shuffle=False)\n        Xb_aug = next(gX)\n        Yb_aug = next(gY)\n        yield Xb_aug, Yb_aug\n\ntrain_gen = create_aug_gen(make_gen(train_df))\nsteps_per_epoch = len(train_df) // BATCH_SIZE\n\n# ─────────────────────────────────────────────\n# 6) Build U‑Net\n# ─────────────────────────────────────────────\ndef build_unet(input_shape=(IMG_SIZE, IMG_SIZE, 3)):\n    inputs = layers.Input(input_shape)\n    def conv_block(x, filters):\n        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n        return x\n    # encoder\n    c1 = conv_block(inputs, 16); p1 = layers.MaxPooling2D()(c1)\n    c2 = conv_block(p1, 32); p2 = layers.MaxPooling2D()(c2)\n    b  = conv_block(p2, 64)\n    # decoder\n    u1 = layers.UpSampling2D()(b); u1 = layers.concatenate([u1, c2])\n    c3 = conv_block(u1, 32)\n    u2 = layers.UpSampling2D()(c3); u2 = layers.concatenate([u2, c1])\n    c4 = conv_block(u2, 16)\n    outputs = layers.Conv2D(1, 1, activation=\"sigmoid\")(c4)\n    return models.Model(inputs, outputs)\n\nmodel = build_unet()\nmodel.summary()\n\n# ─────────────────────────────────────────────\n# 7) Dice Metric & Callbacks\n# ─────────────────────────────────────────────\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = tf.keras.backend.flatten(y_true)\n    y_pred_f = tf.keras.backend.flatten(y_pred)\n    inter   = tf.keras.backend.sum(y_true_f * y_pred_f)\n    return (2. * inter + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[dice_coef]\n)\n\ncheckpoint = ModelCheckpoint(\n    WEIGHT_PATH, monitor=\"val_dice_coef\",\n    save_best_only=True, save_weights_only=True,\n    mode=\"max\", verbose=1\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_dice_coef\", factor=0.5,\n    patience=3, min_lr=1e-6, mode=\"max\", verbose=1\n)\nearly_stop = EarlyStopping(\n    monitor=\"val_dice_coef\", patience=10,\n    mode=\"max\", verbose=1\n)\ncbs = [checkpoint, reduce_lr, early_stop]\n\n# ─────────────────────────────────────────────\n# 8) Prepare Validation Set in RAM\n# ─────────────────────────────────────────────\ndef load_dataset(df):\n    X, Y = [], []\n    for img_id, rles in zip(df.ImageId, df.RLEs):\n        img = cv2.imread(os.path.join(TRAIN_DIR, img_id))\n        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)).astype(np.float32) / 255.0\n        mask = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n        for rle in rles:\n            mask |= rle_decode(rle, (IMG_SIZE, IMG_SIZE))\n        mask = np.expand_dims(mask, -1).astype(np.float32)\n        X.append(img); Y.append(mask)\n    return np.stack(X), np.stack(Y)\n\nX_val, Y_val = load_dataset(val_df.sample(n=100, random_state=42))\n\n\n# ─────────────────────────────────────────────\n# 9) Train\n# ─────────────────────────────────────────────\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=steps_per_epoch,\n    epochs=EPOCHS,\n    validation_data=(X_val, Y_val),\n    callbacks=cbs\n)\n\n# ─────────────────────────────────────────────\n# 10) Save final Model\n# ─────────────────────────────────────────────\n# load best weights, then save full model\nif os.path.exists(WEIGHT_PATH):\n    model.load_weights(WEIGHT_PATH)\nmodel.save(OUTPUT_MODEL)\nprint(\"✅ Full model saved to\", OUTPUT_MODEL)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T21:29:29.509339Z","iopub.execute_input":"2025-04-16T21:29:29.509773Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │            \u001b[38;5;34m448\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │          \u001b[38;5;34m2,320\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m4,640\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m9,248\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │              \u001b[38;5;34m0\u001b[0m │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m36,928\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ up_sampling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mUpSampling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m96\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ up_sampling2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│                           │                        │                │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m27,680\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │          \u001b[38;5;34m9,248\u001b[0m │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ up_sampling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n│ (\u001b[38;5;33mUpSampling2D\u001b[0m)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m48\u001b[0m)   │              \u001b[38;5;34m0\u001b[0m │ up_sampling2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │          \u001b[38;5;34m6,928\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m16\u001b[0m)   │          \u001b[38;5;34m2,320\u001b[0m │ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m17\u001b[0m │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ max_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ up_sampling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ up_sampling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│                           │                        │                │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">27,680</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ up_sampling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)            │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ up_sampling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">6,928</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m118,273\u001b[0m (462.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">118,273</span> (462.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m118,273\u001b[0m (462.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">118,273</span> (462.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n\u001b[1m3335/4787\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:58:55\u001b[0m 5s/step - dice_coef: 0.0039 - loss: 0.0397","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"New","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom glob import glob\n\nIMAGE_DIR = 'train_v2/'\nIMG_SIZE = 384\n\n# Load CSV\ntrain_df = pd.read_csv('train_ship_segmentations_v2.csv')\n\n# Remove null masks and consolidate\ntrain_df = train_df.dropna()\ntrain_df['has_ship'] = ~train_df['EncodedPixels'].isna()\n\n# Group all RLEs per image\nrle_df = train_df.groupby('ImageId')['EncodedPixels'].apply(list).reset_index()\nrle_df['ImagePath'] = rle_df['ImageId'].apply(lambda x: os.path.join(IMAGE_DIR, x))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport cv2\n\ndef rle_decode(mask_rle, shape=(768, 768)):\n    mask = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for rle in mask_rle:\n        if not isinstance(rle, str):\n            continue\n        s = list(map(int, rle.split()))\n        starts, lengths = s[0::2], s[1::2]\n        for start, length in zip(starts, lengths):\n            mask[start-1:start-1+length] = 1\n    return mask.reshape(shape).T\n\ndef process_path(image_path, rles):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE]) / 255.0\n\n    # Decode RLE to mask\n    mask = tf.numpy_function(\n        func=lambda rle_list: cv2.resize(np.max([rle_decode([rle.decode(\"utf-8\")]) for rle in rle_list], axis=0), (IMG_SIZE, IMG_SIZE)),\n        inp=[rles],\n        Tout=tf.float32\n    )\n    mask = tf.expand_dims(mask, axis=-1)\n    return image, mask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset(df, batch_size=8, shuffle=True, augment=True):\n    image_paths = df['ImagePath'].values\n    rle_lists = df['EncodedPixels'].apply(lambda rles: [r.encode() for r in rles]).values\n\n    ds = tf.data.Dataset.from_tensor_slices((image_paths, rle_lists))\n    ds = ds.map(lambda path, rle: process_path(path, rle), num_parallel_calls=tf.data.AUTOTUNE)\n\n    if shuffle:\n        ds = ds.shuffle(1000)\n\n    if augment:\n        ds = ds.map(lambda x, y: data_augment(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n\n    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# Example data augmentation\ndef data_augment(img, mask):\n    if tf.random.uniform(()) > 0.5:\n        img = tf.image.flip_left_right(img)\n        mask = tf.image.flip_left_right(mask)\n    if tf.random.uniform(()) > 0.5:\n        img = tf.image.flip_up_down(img)\n        mask = tf.image.flip_up_down(mask)\n    return img, mask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 8\n\n# Optionally: sample smaller set for fast testing\n# rle_df = rle_df.sample(1000, random_state=42)\n\ntrain_ds = create_dataset(rle_df, batch_size=BATCH_SIZE)\n\n# Model compilation\nmodel = unet_model(input_size=(IMG_SIZE, IMG_SIZE, 3))  # your U-Net\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train it\nmodel.fit(train_ds, epochs=10)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}